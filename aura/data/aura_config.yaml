aura: &aura_config
  log-level: info

  # You can enable/disable/filter python warnings here
  # It's useful to disable them if running a full PyPI repo scan or collecting data sets
  # https://docs.python.org/3.7/library/warnings.html#overriding-the-default-filter
  warnings: default

  # Directory location for caching, unset to disable caching
  cache_location: &cache_location ~/.aura_cache

  # Path to the location of the offline PyPI mirror
  # This option can be safely disabled if you don't have a local mirror, some advanced features require this
  mirror: /var/pypi_mirror/pypi/web/

  # Path to the yara rules for scanning data blobs
  # See example_rules.yara for documentation on format and examples
  yara-rules: &yara_rules aura.data.rules.yara

  # Path to the semantic rules used by python code analyzer
  semantic-rules: &semantic_rules aura.data.signatures.yaml

  # This file is needed for typosquatting detections
  pypi_stats: &pypi_stats pypi_stats.json

  reverse_dependencies: &reverse_dependencies reverse_dependencies.json

  # Threshold for package download after which the package is considered not legitimate
  pypi_download_threshold: 10000

  # Default minimum score for outputting scan hits
  # This can be overridden by a specific output type
  min-score: 10

  # You can enable/disable forking for async processing of files here
  # Async processing is using the python's multiprocessing module
  # It has a much bigger performance but it is not very debugger friendly when developing new plugins
  # It is possible that some 3rd party plugins might require synchronous processing due to data pipelines
  async: false

  # Max width for the default text output
  text-output-width: auto

  # Limit heap size of the process
  # 4G
  rlimit-memory: 4294967296

  # Limit maximum file size the framework can create
  # This is also used as a limit when unpacking archive content to prevent for example zip bombs
  # 4G
  rlimit-fsize: 4294967296

  # When extracting an archive, limit the maximum file size that can be extracted
  # This is a safety mechanism for zip bombs
  # Falls back to rlimit-fsize if not configured
  # max-archive-size: 4294967296

  # You can limit the stack (python frames) recursion here
  # python-recursion-limit = 150
  # Aura recursively unpack archives, this specifies the maximum depth of recursion
  max-depth: 5

  # Limit maximum numbers of files that can be processed during one scan
  max-files: 1000

  # Order of AST analysis stages to run on python source code
  ast-stages: &ast_stages
    - convert
    - rewrite
    - ast_pattern_matching
    - taint_analysis
    - readonly

  # Define a maximum number of iterations for a visitor
  # A new iteration is performed over the AST tree each time a property or a node of the AST tree is modified
  # These iterations are performed until the AST tree converges (e.g. no more modifications are performed) or a maximum number of iterations has been reached
  max-ast-iterations: 500

  # This is a prevention against infinite traversals/loops in the AST tree, which puts a hard limit on queue and discards any new node traversals above the limit
  # In case there is a bug, AST tree could be rewritten in a way that creates loops
  # In some rare cases, the source code could just be extremely big which prolongs the processing a lot, especially the taint analysis
  max-ast-queue-size: 100000

  # Minimum blob size (string or bytes) to be extracted from source code
  # into separate file for scanning. This means that if there is a str or bytes
  # inside the source code longer then X characters it will be extracted
  # and inserted to the data analysis pipeline
  min-blob-size: 100

  # Set preferred output format for cli commands that supports it
  # Supported formats: text, json
  output-format: json

  # If defined, a dedicated log file for exceptions and errors would be created
  error-log: "aura_errors.log"

  # Define the threshold of shanon entropy for strings to be reported
  # TODO: be able to define both min & max
  shanon_entropy: 5.0

  # Always produce an informational `Detection` informing on all module imports in the source code
  # This will report any and all modules (code imports) even if there is no semantic pattern defined for them
  always_report_module_imports: false

tags: &tags
  # Filter results that contain only the specified tags
  # Results can also be excluded using "!" to prefix a tag
  # This list would be used for default tag filtering and can be optionally overriden via cli parameter(s) -t
  #- "!test-code"


diff: &diff
  # Threshold after which files are considered to be similar/modified
  similarity_threshold: 0.60
  # Max depth of files to consider for pairing similar files
  # Increasing this significantly impacts performance of fuzzy matching potentially similar files
  depth_limit: 2


pypirc: &pypirc
  # Blacklist values to reduce false positives from the default configurations
  username_blacklist:
    - "empty"
    - "${PYPI_USERNAME}"
    - "None"
  password_blacklist:
    - "empty"
    - "${PYPI_PASSWORD}"
    - "..."
    - "None"
    - "<your test password goes here>"
    - "<your production password goes here>"

severities: &severities
  critical:
    score: 100
    detections:
      - TaintAnomaly
      - LeakingSecret
  high:
    score: 50
    detections:
      - UnpinnedPackage
  medium:
    score: 30
    detections:
      - OutdatedPackage
  low:
    score: 10


score: &scores
  # Score assigned when a package contain a suspicious file inside such as python bytecode (*.pyc)
  contain-suspicious-file: 5

  # Score assigned when a package contain a sensitive file inside such as accidentally including .pypirc
  contain-sensitive-file: 100

  # You can Adjust the following default values for other built-in analyzers here

  # Wheel contains a file which checksum doesn't match with the one listed in RECORDs
  # wheel-invalid-record-checksum: 100

  # wheel-records-missing: 100

  # Wheel contains a setup.py file
  # wheel-contain-setup-py: 100

  # Wheel contains a file that is not listed in RECORDs
  # wheel-file-not-listed-in-records: 10

  # There is a file listed in RECORDs but missing inside the wheel archive
  # wheel-missing-file: 100

  # Valid base 64 blob was found as a string in a source code
  # base-64-blob: 0

  # Archive contains absolute path such as /etc/passwd
  # suspicious-archive-entry-absolute-path: 50

  # Archive contains a file with a parent reference such as ../../../../etc/passwd
  # suspicious-archive-entry-parent-reference: 50

  # File appears to be an archive but can't be opened successfully and/or is corrupted
  # corrupted-archive: 10

  # Archive contain a file greater than the configured maximum archive file size, e.g. zip bomb
  # archive-file-size-exceeded: 100

  # Archive contain a member that is a link. This can lead to tarbomb or overwriting files outside the extraction directory
  # archive-member-is-link: 100

  # XML contain an entity which can be used for billion laughs attacks and similar
  # malformed-xml-entities: 100

  # XML contain DTD
  # malformed-xml-dtd: 20

  # XML contain external reference which can be used to access external resources including files on disk
  # malformed-xml-external-reference: 100

  # PyPI requirement is not pinned
  # requirement-unpinned: 10

  # PyPI requirement is outdated
  # requirement-outdated: 5

  # PyPI requirement is not valid, e.g. parsing failed
  # requirement-invalid: 0

  # PyPI requirement points to a remote URL
  # requirement-remote-url: 20

cache: &cache
  # Mode can be one of:
  # - "ask": ask if cache should be purged when running some commands (aura info, aura update ...)
  # - "auto": same as ask but it would purge the cage automatically without asking on specific commands
  # - "always": purge cache after every operation. This is same as "auto" + other non-standard operations including `aura scan`, `aura diff`
  mode: auto
  max-size: 1G
  expiration:
    default: 72  # 3 days


interpreters: &interpreters
  # Configure python interpreters for parsing AST code
  # `python2` must point to the py2.7+ version (versions under 2.7 are not supported but might work)
  # `python3` must point to the py3.6+ or ideally py3.7+ due to compatibility
  # All other interpreters are optional, AST parsing will try them in the defined order

  native: native
  python2: python2
